{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 46-926, Statistical Machine Learning 1: Homework 2\n",
    "\n",
    "*Author*  : Jordan Giebas <br>\n",
    "*Due Date*: January 28th, 2018 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a)\n",
    "Plot the loss function given by,\n",
    "\n",
    "$$ \\mathcal{L}(Y,f(X)) = b\\big(e^{a(Y-f(X)} - a(Y-f(X)) - 1\\big) $$\n",
    "\n",
    "for $z = Y-f(X)$, $z \\in [-2,2]$, and where $(a,b) = (1.1,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x108adc208>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c80a6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define the loss function\n",
    "def loss_f(z):\n",
    "    \n",
    "    return ( 2*(np.exp(1.1*z) - 1.1*z - 1) )\n",
    "\n",
    "# Vectorize the function to perform element wise\n",
    "v_loss_f = np.vectorize(loss_f) \n",
    "\n",
    "z_axis = np.linspace(-2, 2)\n",
    "y_axis = v_loss_f(z_axis)\n",
    "\n",
    "\n",
    "plt.xlabel(\"z = Y - f(X)\")\n",
    "plt.ylabel(\"Loss(z)\")\n",
    "plt.title(\"Loss(z) vs. z = Y-f(X)\")\n",
    "plt.scatter(z_axis,y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**: <br>\n",
    "The loss function is interesting. It is sensible that when $z$ is close to zero, that the loss function is too. The interesting observation is the extremeties. When your prediction $f(X)$ highly underestimating $Y$, the loss function is much greater than when your prediction $f(X)$ is overestimating $Y$. <br>\n",
    "**Usage**: <br>\n",
    "When you want to punish the algorithm for underestimating the true value severly, but only marginally punish the algo for overestimating, this is a very good likelihood function to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "Determine the f(X) that will minimize the expected loss, using the loss function provided above. <br>\n",
    "\n",
    "Following the notes from class, we first use iterated conditioning and condition on a particular value of $X$. Then, optimize by solving the derivative with respect to $f(x)$ equal to zero. <br>\n",
    "\n",
    "$$ \\mathbb{E}(\\mathcal{L}(Y,f(X))) = \\mathbb{E}\\big[\\mathbb{E}\\big[\\mathcal{L}(Y,f(X))|X=x\\big]\\big]$$\n",
    "\n",
    "Focusing only on the inside conditional expectation and plugging in $\\mathcal{L}(Y,f(X))$,\n",
    "\n",
    "$$ \\mathbb{E}\\big[b\\big(e^{a(Y-f(X)} - a(Y-f(X)) - 1\\big)|X=x\\big] $$\n",
    "$$ b\\mathbb{E}\\big[\\big(e^{a(Y-f(X)} - a(Y-f(X)) - 1\\big)|X=x\\big] = b\\big(-a\\mathbb{E}(Y|X=x) - 1 + \\mathbb{E}(e^{a(Y-f(X))}|X=x) + a\\mathbb{E}(f(X)|X=x)\\big) $$\n",
    "\n",
    "Luckily the exponential function is a homomorphism and we can separate the arguments then condition on $e^{f(X)}|X=x$. All together we see,\n",
    "\n",
    "$$ b\\big(-a\\mathbb{E}(Y|X=x) - 1 + e^{-af(x)}\\mathbb{E}(e^{aY}|X=x) + af(x)\\big) $$\n",
    "\n",
    "Differentiating the inside with respect to $f(X)$, setting equal to zero, and solving, yields the following result for $f(x)$:\n",
    "\n",
    "$$ f(x) = \\frac{lg\\big(\\mathbb{E}(e^{aY}|X=x)\\big)}{a} $$\n",
    "\n",
    "### Part(c)\n",
    "If the conditional distribution $Y|X=x ~ N(\\beta x, \\sigma^{2})$, then what is the form of the optimal estimator $f(x)$? <br>\n",
    "\n",
    "Since we know the conditional distribtuion, we can evaluate the conditional expectation in the final result of **Part(b)**. We recognize that this is the moment generating function for $Y$, evaluated at $t=a$. Hence,\n",
    "\n",
    "$$ f(x) = \\frac{lg\\big(e^{a\\beta x + \\frac{1}{2}a\\sigma^2)}\\big)}{a} = \\beta x + \\frac{a\\sigma^2}{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part(d)\n",
    "For part(d), I copy and paste the code from `asymm_loss.py` into the following cell and run, for ease in having everything local in the jupyter notebook (although it is possible to call the function in a python file from within this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss of the conditional expectation: 13.27\n",
      "Average loss of your method: 3.98\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "#Set some parameters\n",
    "beta = 0.5\n",
    "b = 2\n",
    "sigma = 2\n",
    "a = 1\n",
    "\n",
    "#Define the loss function, where z = y - yhat\n",
    "def loss(z):\n",
    "    return b*(np.exp(a*z)-a*z-1)\n",
    "\n",
    "#Estimation functions\n",
    "#Estimation using the conditional expectation of Y|X\n",
    "def f_condexp(x):\n",
    "    return beta*x\n",
    "\n",
    "# TODO: Put your function in here.  You can reference a,b,sigma, and it will just pull them from\n",
    "# the outside namespace\n",
    "def f_yours(x):\n",
    "    return beta*x + a*(sigma**2)/2.0\n",
    "\n",
    "#Simulation to see how you do\n",
    "reps = 1000\n",
    "\n",
    "#Just generate the X variables normally.  We don't really care\n",
    "x = norm.rvs(size=reps, loc=0, scale=1)\n",
    "\n",
    "#Generate the Y variables from our normal model\n",
    "y = norm.rvs(size=reps, loc=x*beta, scale=sigma)\n",
    "\n",
    "#Calculate the fitted values for each method\n",
    "yhat_condexp = np.apply_along_axis(f_condexp, 0, x)\n",
    "yhat_yours = np.apply_along_axis(f_yours, 0, x)\n",
    "\n",
    "#Compute the losses\n",
    "condexp_losses = np.apply_along_axis(loss, 0, y-yhat_condexp)\n",
    "your_losses = np.apply_along_axis(loss, 0, y-yhat_yours)\n",
    "\n",
    "print(\"Average loss of the conditional expectation:\",\n",
    "      round(np.mean(condexp_losses),2))\n",
    "\n",
    "print(\"Average loss of your method:\", \n",
    "      round(np.mean(your_losses),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average loss of the estimator derived in Part(c) is substantially lower than the average loss of the conditional expectation. <br>In terms of bias-variance tradeoff, we are introducing some bias but this is marginal relative to the amount of variance reduction we receive. Hence, we see a lower expected loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a)\n",
    "\n",
    "The ridge regression coefficients are given by (pardon my leaving out the hat on $\\beta$, it doesn't look right when I compile the latex in the jupyter notebook),\n",
    "\n",
    "$$ {\\beta}^{ridge} = \\underset{\\beta \\in \\mathbb{R}^{p}}{\\operatorname{arg\\,max}} ||y-X\\beta||_2^2 + \\lambda||\\beta||_2^2$$\n",
    "\n",
    "Where $y \\in \\mathbb{R}^n$ is our response vector, $X \\in \\mathbb{R}^{nxp}$ is our prediction matrix, and $\\beta \\in \\mathbb{R}^p$ is our coefficient vector. \n",
    "\n",
    "We follow the structure of the problem, we have block matrices and vectors given as $\\widetilde{y} = \\bigg[ \\begin{smallmatrix} y\\\\0 \\end{smallmatrix} \\bigg] \\in \\mathbb{R}^{n+p}$, $\\widetilde{X} = \\bigg[ \\begin{smallmatrix} X\\\\ \\sqrt\\lambda I \\end{smallmatrix}\\bigg] \\in \\mathbb{R}^{(n+p)xp}$ where $I \\in \\mathbb{R}^{pxp}$ is the identity matrix. <br>\n",
    "\n",
    "We know from basic linear regression (OLS) that the coefficients of regressing an observation vector onto the predicition matrix is given by \n",
    "\n",
    "$$ \\beta^{OLS} = \\big(X^{T}X\\big)^{-1}X^{T}y $$\n",
    "\n",
    "Regressing our block response vector on our block prediction matrix yields the following,\n",
    "\n",
    "$$ \\beta = \\big(\\widetilde{X}^{T}\\widetilde{X}\\big)^{-1}\\widetilde{X}^{T}\\widetilde{y} $$\n",
    "\n",
    "Performing a little matrix multiplication simplifies the result. Consider the following,\n",
    "\n",
    "$$ \\widetilde{X}^{T}\\widetilde{X} = \\bigg[\\begin{smallmatrix} X\\\\ \\sqrt\\lambda I \\end{smallmatrix}\\bigg]^{T}\\bigg[ \\begin{smallmatrix} X\\\\ \\sqrt\\lambda I \\end{smallmatrix}\\bigg] = \\bigg[ \\begin{smallmatrix} X^{T}&\\sqrt\\lambda I \\end{smallmatrix}\\bigg] \\bigg[ \\begin{smallmatrix} X\\\\ \\sqrt\\lambda I \\end{smallmatrix}\\bigg] = X^{T}X - \\lambda I $$\n",
    "\n",
    "Also, \n",
    "\n",
    "$$ \\widetilde{X}^{T} \\widetilde{y} = \\bigg[ \\begin{smallmatrix} X^{T}&\\sqrt\\lambda I \\end{smallmatrix}\\bigg] \\bigg[ \\begin{smallmatrix} y\\\\0 \\end{smallmatrix} \\bigg] = X^{T}X - \\lambda I \\vec{0} = X^{T}X $$\n",
    "\n",
    "Hence, the beta from regressing the block response vector on the block predicition vector takes the form\n",
    "\n",
    "$$ \\beta = (X^{T}X + \\lambda I)^{-1}X^{T}y $$\n",
    "\n",
    "which as we saw, is precisely the solution to the ridge regression problem stated above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "We assume that $\\lambda > 0$ so that $\\sqrt \\lambda$ is well defined. We would like to prove that $\\widetilde{X}$ must have full column-rank. It suffices to show that each of the columns of $\\widetilde{X}$ are linearly independent. Firstly, let's write out the entries of $\\widetilde{X}$.\n",
    "\n",
    "\n",
    "$$ \\widetilde{X} = \\begin{bmatrix} \n",
    "    x_{11} & x_{12} & \\dots & x_{1p}\\\\\n",
    "    x_{21} & x_{22} & \\dots & x_{2p}\\\\\n",
    "    \\vdots & \\vdots & \\dots & \\vdots\\\\\n",
    "    x_{n1} & x_{n2} & \\dots & x_{np}\\\\\n",
    "    \\sqrt\\lambda & 0 & \\dots & 0 \\\\\n",
    "    0 & \\sqrt\\lambda & \\dots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots\\\\\n",
    "    0 & 0 & \\dots & \\sqrt\\lambda\n",
    "    \\end{bmatrix} $$\n",
    "    \n",
    "Now define the collection of vectors $\\{\\vec{v}_i\\}_{i=1}^{p}$ as the columns of the $\\widetilde{X}$, where each $\\vec{v}_i \\in \\mathbb{R}^{n+p}$. <br>\n",
    "\n",
    "We would like to show that this collection of vectors is linearly independent. Hence, we must show that for any linear combination equal to zero, $\\sum_{i=1}^{p} \\alpha_{i}\\vec{v}_i = \\vec{0}$, that this implies $\\alpha_{i} = \\vec{0}, \\forall i$. <br>\n",
    "\n",
    "Consider the following linear combination set equal to zero,\n",
    "\n",
    "$$\\sum_{i=1}^{p} \\alpha_{i}\\vec{v}_i = \\alpha_{1}\\begin{bmatrix} x_{11} \\\\ \n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 x_{n1} \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 \\sqrt\\lambda \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 0 \\\\\n",
    "                                                                 0 \\end{bmatrix} + \n",
    "                                       \\alpha_{2}\\begin{bmatrix} x_{12} \\\\ \n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 x_{n2} \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 0 \\\\\n",
    "                                                                 \\sqrt\\lambda \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 0 \\end{bmatrix} + \n",
    "                                        \\dotsi +\n",
    "                                        \\alpha_{p}\\begin{bmatrix} x_{1p} \\\\ \n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 x_{np} \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 0 \\\\\n",
    "                                                                 0 \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 \\sqrt\\lambda \\end{bmatrix} = \n",
    "                                        \\begin{bmatrix} \\alpha_{1} x_{11} \\\\ \n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 \\alpha_{1} x_{n1} \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 \\alpha_{1} \\sqrt\\lambda \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 0 \\\\\n",
    "                                                                 0 \\end{bmatrix} + \n",
    "                                       \\begin{bmatrix} \\alpha_{2} x_{12} \\\\ \n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 \\alpha_{2} x_{n2} \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 0 \\\\\n",
    "                                                                 \\alpha_{2} \\sqrt\\lambda \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 0 \\end{bmatrix} + \n",
    "                                        \\dotsi +\n",
    "                                        \\begin{bmatrix} \\alpha_{p} x_{1p} \\\\ \n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 \\alpha_{p} x_{np} \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 0 \\\\\n",
    "                                                                 0 \\\\\n",
    "                                                                 \\vdots \\\\\n",
    "                                                                 \\alpha_{p} \\sqrt\\lambda \\end{bmatrix} = \n",
    "                                        \\begin{bmatrix} 0 \\\\ \n",
    "                                                        \\vdots \\\\\n",
    "                                                        0 \\\\\n",
    "                                                        \\vdots \\\\\n",
    "                                                        0 \\\\\n",
    "                                                        0 \\\\\n",
    "                                                        \\vdots \\\\\n",
    "                                                        0 \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "This is clearly a system of equations, but let's focus our attention on the $p^{th}, (p+1)^{th}, ..., (n+p)^{th}$ rows. Solving each of these equations clearly shows that $\\{\\alpha_{i}\\}_{i=1}^p = 0, \\forall i$. Hence, we conclude that the columns of $\\widetilde{X}$ are linearly independent, and it follows immediately that $\\widetilde{X}$ has full column-rank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c)\n",
    "\n",
    "In Part(a) we confirmed that for $I \\in \\mathbb{R}^{pxp}$, the $\\beta^{ridge}$ is given by (again pardon the omissino of the hat),\n",
    "\n",
    "$$ \\beta^{ridge} = (X^{T}X + \\lambda I)^{-1}X^{T}y $$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$ a^{T}\\beta^{ridge} = a^{T}\\bigg[(X^{T}X + \\lambda I)^{-1}X^{T}y\\bigg] $$\n",
    "\n",
    "By associativity,\n",
    "\n",
    "$$ a^{T}\\beta^{ridge} = \\bigg[a^{T}(X^{T}X + \\lambda I)^{-1}X^{T}\\bigg]y $$\n",
    "\n",
    "I'm not sure how much detail is needed, I see it two ways. The first is that this is clearly a linear system in $y$ because matrix multiplication is a linear transformation. The second is that, we're told that $a \\in \\mathbb{R}^p$ as well. Hence, $a^{T} \\in \\mathbb{R}^{1xp}$. It follows that, $\\bigg[a^{T}(X^{T}X + \\lambda I)^{-1}X^{T}\\bigg] \\in \\mathbb{R}^{1xn}$. Hence, when this quantity is multiplied by $y \\in \\mathbb{R}^{n}$, we will get a scalar quantity. I'm not sure exactly what more is being asked, hopefully this is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted as a separate .Rmd file - there is a problem installing `rpy2` with the new OS \"macOS High Sierra\" if you have R --version 3.4+, so I can't simply put this all here. (It's a problem regarding clang and g++ compilers from the C-API underneath the hood for Python to R)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
